# AACR-Bench: A Multi-lingual Repository-level Context-aware Automated Code Review Benchmark
<!-- ËøôÊòØ‰∏ÄÂº†ÂõæÁâáÔºåocr ÂÜÖÂÆπ‰∏∫Ôºö -->
![](https://img.shields.io/badge/License-Apache_2.0-blue.svg)<!-- ËøôÊòØ‰∏ÄÂº†ÂõæÁâáÔºåocr ÂÜÖÂÆπ‰∏∫Ôºö -->
![](https://img.shields.io/badge/Dataset-v1.0-green.svg)<!-- ËøôÊòØ‰∏ÄÂº†ÂõæÁâáÔºåocr ÂÜÖÂÆπ‰∏∫Ôºö -->
![](https://img.shields.io/badge/Languages-10-orange.svg)<!-- ËøôÊòØ‰∏ÄÂº†ÂõæÁâáÔºåocr ÂÜÖÂÆπ‰∏∫Ôºö -->
![](https://img.shields.io/badge/PRs-200-red.svg)<!-- ËøôÊòØ‰∏ÄÂº†ÂõæÁâáÔºåocr ÂÜÖÂÆπ‰∏∫Ôºö -->

## üìã Introduction
AACR-Bench is a multi-lingual, repository-level context-aware code review benchmark dataset designed to evaluate the performance of Large Language Models (LLMs) in automated code review tasks. The dataset contains 200 real Pull Requests from 50 active open-source projects, covering 10 mainstream programming languages. Each instance not only includes code changes but also preserves comprehensive repository context, faithfully reproducing the entire code review process. Through human-LLM collaborative review and multi-round expert annotation, we ensure high-quality and comprehensive data.

### Main Applications
+ Evaluating code review models' issue detection capabilities
+ Assessing the quality and feasibility of review suggestions
+ Testing cross-language and cross-project generalization abilities
+ Fine-grained analysis of context-awareness capabilities

## ‚ú® Core Features
### üåê Multi-language Coverage
Covers 10 mainstream programming languages: Python, Java, JavaScript, TypeScript, Go, Rust, C++, C#, Ruby, PHP

### üìÅ Repository-level Context
+ Preserves complete project structure and dependency information
+ Supports cross-file reference analysis

### ü§ñ Human Expert + LLM Enhanced Annotation
+ Human expert initial review + systematic LLM supplementation
+ Identifies subtle issues and potential improvements
+ Multi-round expert review ensures comment quality

### üìä Comprehensive Evaluation Metrics
+ **Precision**: Comment quality and line-level positioning accuracy
+ **Recall**: Completeness of issue discovery
+ **Noise Rate**: Invalid comment identification
+ **Multi-dimensional Analysis**: Support for language and issue type statistics

## üöÄ Quick Start (TBD)
### Installation
```bash
# Installation instructions coming soon
```

### Download Dataset
```bash
# Download instructions coming soon
```

### Run Evaluation
```bash
# Evaluation instructions coming soon
```

## üìà Dataset Overview
### Statistics
| Metric | Count |
| --- | --- |
| Pull Requests | 200 |
| Programming Languages | 10 |
| Source Projects | 50 |
| Total Review Comments | 1,505 |


### Data Format
```json
{
  "type": "array",
  "item": {
    "change_line_count": {"type": "integer", "description": "Number of modified lines"},
    "project_main_language": {"type": "string", "description": "Project's main language"},
    "source_commit": {"type": "string", "description": "Source commit hash"},
    "target_commit": {"type": "string", "description": "Target commit hash"},
    "githubPrUrl": {"type": "string", "description": "GitHub PR URL"},
    "comments": {
      "type": "array",
      "description": "Annotated review comments",
      "items": {
        "type": "object",
        "properties": {
          "is_ai_comment": {"type": "boolean", "description": "Whether it's an AI comment"},
          "note": {"type": "string", "description": "Review comment in English"},
          "path": {"type": "string", "description": "File path"},
          "side": {"type": "string", "description": "Comment position"},
          "source_model": {"type": "string", "description": "Source model"},
          "from_line": {"type": "integer", "description": "Starting line number"},
          "to_line": {"type": "integer", "description": "Ending line number"},
          "category": {"type": "string", "description": "Comment category"},
          "context": {"type": "string", "description": "Comment scope"}
        }
      }
    }
  }
}
```

## üìè Evaluation Metrics
We employ a multi-dimensional metric system to comprehensively evaluate code review model performance. For complete metric definitions, calculation methods, and per-language statistics, please refer to [docs/METRICS.md](docs/METRICS.md).

### Core Metrics
| Metric | Description | Formula |
| --- | --- | --- |
| **Precision** | Proportion of valid comments generated by the model | Valid matches / Total generated |
| **Recall** | Ability to discover issues in the annotation set | Valid matches / Valid in dataset |
| **Line Precision** | Ability to precisely locate code lines | Line matches / Total generated |
| **Noise Rate** | Proportion of invalid or incorrect comments | Unmatched / Total generated |


### Benchmark Results (TBD)
| Model | Precision | Recall | Line Precision | Noise Rate |
| --- | --- | --- | --- | --- |
| Qwen |  |  |  |  |
| Claude-4 |  |  |  |  |
| Deepseek |  |  |  |  |


## ü§ù Contributing
We welcome community contributions! To contribute to AACR-Bench, please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feat/add-new-prs`)
3. Commit your changes (`git commit -m 'feat: add new PRs for rust'`)
4. Push to the branch (`git push origin feat/add-new-prs`)
5. Create a Pull Request

For detailed contributing guidelines, please refer to [CONTRIBUTING.md](CONTRIBUTING.md)

## üë• Authors and Maintainers
| Name | GitHub                                            | Area | Responsibilities |
| --- |---------------------------------------------------| --- | --- |
| Zhengfeng Li | [@lizhengfeng](https://github.com/lizhengfeng101) | Project Lead | Overall architecture design, technical direction |
| Lei Zhang | [@zhanglei](https://github.com/TongWu-ZL) | Data Architecture | Evaluation framework, metric system, performance optimization |
| Yongda Yu | [@yuyongda](https://github.com/inkeast) | Evaluation System | Data schema design, evaluation protocol, quality standards |
| Xinxin Guo | [@guoxinxin](https://github.com/guoxinxin125) | Annotation Platform | Annotation system development, workflow design, quality assurance |
| Minghui Yu | [@yuminghui](https://github.com/yuminghui) | AI Enhancement | LLM annotation pipeline, model selection, prompt optimization |
| Zhengqi Zhuang | [@zhuangzhengqi](https://github.com/ZhengqiZhuang) | Engineering | CI/CD pipeline, automated testing, deployment scripts |


## üìÑ License
This project is licensed under the Apache License 2.0. See the [LICENSE](LICENSE) file for details.

## üìö Citation (TBD)
If you use AACR-Bench in your research, please cite our paper:

```plain
@article{liu2026aacrbench,
  title={AACR-Bench: A Multi-lingual Repository-level Context-aware Automated Code Review Benchmark},
  author={Li, Zhengfeng and Zhang, Lei and Yu, Yongda and Guo, Xinxin and Yu, Minghui and Zhuang, Zhengqi},
  journal={arXiv preprint arXiv:2026.xxxxx},
  year={2026},
  url={https://arxiv.org/abs/2026.xxxxx}
}
```

## üó∫Ô∏è Roadmap
- [x] v1.0 (2026.01): Initial release - 200 PRs, 10 languages
- [ ] 

## <font style="color:rgb(31, 35, 40);">üåü</font><font style="color:rgb(31, 35, 40);"> </font>Acknowledgments
+ Thanks to all contributors who participated in data annotation, especially core contributors who completed 15+ valid annotations. See the full list at [docs/ANNOTATORS.md](docs/ANNOTATORS.md).
+ Thanks to the maintainers of open-source projects who provided the original PR data.